#!/usr/bin/env python3
"""
caption_recovery.py - ì¸ìŠ¤íƒ€/ë¸”ë¡œê·¸ ìº¡ì…˜ ë³µêµ¬ ë° ë…¸ì…˜ êµ¬ì¡°í™”
"""

import os
import sys
import json
import re
import time
import requests
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

PROJECT_ROOT = Path(__file__).parent.parent
load_dotenv(PROJECT_ROOT / ".env")

NOTION_API_KEY = os.getenv("NOTION_API_KEY")
NOTION_DATABASE_ID = os.getenv("NOTION_DATABASE_ID")
NOTION_API_VERSION = "2022-06-28"

CONTENTS_DIR = PROJECT_ROOT / "01_contents"
# 2026-02-13: í”Œë« êµ¬ì¡° - STATUS_DIRS ì œê±°
# STATUS_DIRS = ["4_posted", "3_approved", "2_body_ready", "1_cover_only"]


def get_headers():
    return {
        "Authorization": f"Bearer {NOTION_API_KEY}",
        "Content-Type": "application/json",
        "Notion-Version": NOTION_API_VERSION
    }


def fetch_all_pages():
    """ë…¸ì…˜ DBì—ì„œ ëª¨ë“  í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
    pages = []
    has_more = True
    start_cursor = None

    while has_more:
        url = f"https://api.notion.com/v1/databases/{NOTION_DATABASE_ID}/query"
        body = {"sorts": [{"property": "ë²ˆí˜¸", "direction": "ascending"}]}
        if start_cursor:
            body["start_cursor"] = start_cursor

        response = requests.post(url, headers=get_headers(), json=body)
        if response.status_code != 200:
            print(f"âŒ API ì˜¤ë¥˜: {response.status_code}")
            break

        data = response.json()
        pages.extend(data.get("results", []))
        has_more = data.get("has_more", False)
        start_cursor = data.get("next_cursor")

    return pages


def find_content_folder(num: int) -> Path:
    """ë²ˆí˜¸ë¡œ ì½˜í…ì¸  í´ë” ì°¾ê¸°"""
    # 2026-02-13: í”Œë« êµ¬ì¡° - contents/ ì§ì ‘ ìŠ¤ìº”
    num_str = f"{num:03d}"
    for item in CONTENTS_DIR.iterdir():
        if item.is_dir() and item.name.startswith(num_str):
            return item
    return None


def get_property_value(props, prop_name, prop_type):
    """ë…¸ì…˜ í”„ë¡œí¼í‹° ê°’ ì¶”ì¶œ"""
    prop = props.get(prop_name, {})

    if prop_type == "url":
        return prop.get("url", "")
    elif prop_type == "number":
        return prop.get("number")
    elif prop_type == "title":
        title_arr = prop.get("title", [])
        return title_arr[0].get("plain_text", "") if title_arr else ""
    elif prop_type == "rich_text":
        text_arr = prop.get("rich_text", [])
        return text_arr[0].get("plain_text", "") if text_arr else ""

    return None


def scrape_instagram_caption(url: str) -> str:
    """ì¸ìŠ¤íƒ€ê·¸ë¨ ìº¡ì…˜ ìŠ¤í¬ë˜í•‘ (ê¸°ë³¸ ë°©ì‹)"""
    if not url or "instagram.com" not in url:
        return ""

    try:
        # ëª¨ë°”ì¼ User-Agent ì‚¬ìš©
        headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15",
            "Accept": "text/html,application/xhtml+xml",
            "Accept-Language": "ko-KR,ko;q=0.9",
        }

        response = requests.get(url, headers=headers, timeout=10)

        if response.status_code != 200:
            return ""

        html = response.text

        # meta descriptionì—ì„œ ìº¡ì…˜ ì¶”ì¶œ ì‹œë„
        # íŒ¨í„´: <meta property="og:description" content="..."/>
        match = re.search(r'<meta property="og:description" content="([^"]*)"', html)
        if match:
            caption = match.group(1)
            # HTML ì—”í‹°í‹° ë””ì½”ë”©
            caption = caption.replace("&amp;", "&")
            caption = caption.replace("&lt;", "<")
            caption = caption.replace("&gt;", ">")
            caption = caption.replace("&quot;", '"')
            caption = caption.replace("&#x27;", "'")
            return caption

        # ëŒ€ì²´ íŒ¨í„´
        match = re.search(r'"caption":"([^"]*)"', html)
        if match:
            return match.group(1).encode().decode('unicode_escape')

    except Exception as e:
        print(f"      ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")

    return ""


def scrape_naver_blog_content(url: str) -> str:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ ì¶”ì¶œ"""
    if not url or "blog.naver.com" not in url:
        return ""

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
        }

        response = requests.get(url, headers=headers, timeout=10)

        if response.status_code != 200:
            return ""

        html = response.text

        # iframe src ì¶”ì¶œ
        match = re.search(r'src="(https://blog\.naver\.com/PostView\.naver[^"]*)"', html)
        if match:
            iframe_url = match.group(1)
            response = requests.get(iframe_url, headers=headers, timeout=10)
            html = response.text

        # ë³¸ë¬¸ ì¶”ì¶œ (se-main-container)
        match = re.search(r'<div class="se-main-container"[^>]*>(.*?)</div>\s*</div>\s*</div>', html, re.DOTALL)
        if match:
            content = match.group(1)
            # HTML íƒœê·¸ ì œê±°
            content = re.sub(r'<[^>]+>', '', content)
            content = content.strip()
            return content[:2000]  # ìµœëŒ€ 2000ì

    except Exception as e:
        print(f"      ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")

    return ""


def create_toggle_structure(page_id: str, food_name: str, insta_data: dict, blog_data: dict):
    """ë…¸ì…˜ í˜ì´ì§€ì— í† ê¸€ êµ¬ì¡° ìƒì„±"""

    children = []

    # ì¸ìŠ¤íƒ€ í† ê¸€
    insta_children = []
    if insta_data.get("caption"):
        insta_children.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [{"type": "text", "text": {"content": insta_data["caption"][:2000]}}]
            }
        })
    if insta_data.get("images"):
        for img_path in insta_data["images"][:10]:
            insta_children.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": f"ğŸ“· {img_path}"}}]
                }
            })

    if insta_children:
        children.append({
            "object": "block",
            "type": "toggle",
            "toggle": {
                "rich_text": [{"type": "text", "text": {"content": "ğŸ“¸ ì¸ìŠ¤íƒ€"}}],
                "children": insta_children[:100]
            }
        })

    # ë¸”ë¡œê·¸ í† ê¸€
    blog_children = []
    if blog_data.get("caption"):
        blog_children.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [{"type": "text", "text": {"content": blog_data["caption"][:2000]}}]
            }
        })
    if blog_data.get("images"):
        for img_path in blog_data["images"][:10]:
            blog_children.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": f"ğŸ“· {img_path}"}}]
                }
            })

    if blog_children:
        children.append({
            "object": "block",
            "type": "toggle",
            "toggle": {
                "rich_text": [{"type": "text", "text": {"content": "ğŸ“ ë¸”ë¡œê·¸"}}],
                "children": blog_children[:100]
            }
        })

    if not children:
        return False

    # ê¸°ì¡´ ë¸”ë¡ ê°€ì ¸ì˜¤ê¸°
    url = f"https://api.notion.com/v1/blocks/{page_id}/children"
    response = requests.get(url, headers=get_headers())

    if response.status_code == 200:
        existing_blocks = response.json().get("results", [])

        # ê¸°ì¡´ í† ê¸€ ì‚­ì œ (ğŸ“¸ ì¸ìŠ¤íƒ€, ğŸ“ ë¸”ë¡œê·¸)
        for block in existing_blocks:
            if block.get("type") == "toggle":
                toggle_text = ""
                rich_text = block.get("toggle", {}).get("rich_text", [])
                if rich_text:
                    toggle_text = rich_text[0].get("plain_text", "")

                if toggle_text in ["ğŸ“¸ ì¸ìŠ¤íƒ€", "ğŸ“ ë¸”ë¡œê·¸"]:
                    delete_url = f"https://api.notion.com/v1/blocks/{block['id']}"
                    requests.delete(delete_url, headers=get_headers())

    # ìƒˆ í† ê¸€ ì¶”ê°€
    url = f"https://api.notion.com/v1/blocks/{page_id}/children"
    payload = {"children": children}

    response = requests.patch(url, headers=get_headers(), json=payload)
    return response.status_code == 200


def update_checkboxes(page_id: str, insta_caption: bool, blog_caption: bool):
    """ì²´í¬ë°•ìŠ¤ ì—…ë°ì´íŠ¸"""
    url = f"https://api.notion.com/v1/pages/{page_id}"
    payload = {
        "properties": {
            "insta_caption": {"checkbox": insta_caption},
            "blog_caption": {"checkbox": blog_caption},
        }
    }
    response = requests.patch(url, headers=get_headers(), json=payload)
    return response.status_code == 200


def main():
    print("â”" * 60)
    print("ğŸ“‹ ìº¡ì…˜ ë³µêµ¬ ë° ë…¸ì…˜ êµ¬ì¡°í™”")
    print(f"   ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("â”" * 60)

    # 1. ë…¸ì…˜ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
    print("\nğŸ“¥ ë…¸ì…˜ í˜ì´ì§€ ë¡œë“œ ì¤‘...")
    pages = fetch_all_pages()
    print(f"   ì´ {len(pages)}ê°œ í˜ì´ì§€")

    # 2. ë¶„ë¥˜
    insta_pages = []
    blog_pages = []

    for page in pages:
        props = page.get("properties", {})
        num = get_property_value(props, "ë²ˆí˜¸", "number")
        name = get_property_value(props, "ì œëª©", "title") or get_property_value(props, "Name", "title")
        insta_url = get_property_value(props, "ì¸ìŠ¤íƒ€URL", "url") or get_property_value(props, "insta_url", "url")
        blog_url = get_property_value(props, "ë¸”ë¡œê·¸URL", "url") or get_property_value(props, "blog_url", "url")

        if insta_url:
            insta_pages.append({
                "page_id": page["id"],
                "num": num,
                "name": name,
                "url": insta_url
            })

        if blog_url:
            blog_pages.append({
                "page_id": page["id"],
                "num": num,
                "name": name,
                "url": blog_url
            })

    print(f"   ì¸ìŠ¤íƒ€ URL ìˆìŒ: {len(insta_pages)}ê°œ")
    print(f"   ë¸”ë¡œê·¸ URL ìˆìŒ: {len(blog_pages)}ê°œ")

    # 3. ì¸ìŠ¤íƒ€ ìº¡ì…˜ ë³µêµ¬
    print("\n" + "â”" * 60)
    print("ğŸ“¸ ì¸ìŠ¤íƒ€ ìº¡ì…˜ ë³µêµ¬")
    print("â”" * 60)

    insta_success = 0
    insta_fail = 0

    for i, item in enumerate(insta_pages):
        num = item["num"]
        name = item["name"]
        url = item["url"]
        page_id = item["page_id"]

        print(f"\n[{i+1}/{len(insta_pages)}] #{num} {name}")
        print(f"   URL: {url[:50]}...")

        # í´ë” ì°¾ê¸°
        folder = find_content_folder(num)
        if not folder:
            print(f"   âš ï¸ í´ë” ì—†ìŒ")
            insta_fail += 1
            continue

        # 2026-02-13: í”Œë« êµ¬ì¡° ë°˜ì˜
        insta_dir = folder / "01_Insta&Thread"
        insta_dir.mkdir(exist_ok=True)
        caption_file = insta_dir / "caption.txt"

        # ì´ë¯¸ ìº¡ì…˜ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if caption_file.exists() and caption_file.stat().st_size > 10:
            print(f"   âœ… ìº¡ì…˜ ì´ë¯¸ ì¡´ì¬ ({caption_file.stat().st_size}ì)")
            insta_success += 1

            # ë…¸ì…˜ ì—…ë°ì´íŠ¸
            caption = caption_file.read_text()
            images = [f.name for f in insta_dir.iterdir() if f.suffix.lower() in [".jpg", ".png", ".webp"]]
            create_toggle_structure(page_id, name, {"caption": caption, "images": images}, {})
            update_checkboxes(page_id, True, False)
            continue

        # ìŠ¤í¬ë˜í•‘
        caption = scrape_instagram_caption(url)

        if caption and len(caption) > 10:
            caption_file.write_text(caption)
            print(f"   âœ… ìº¡ì…˜ ì €ì¥ ({len(caption)}ì)")
            insta_success += 1

            # ë…¸ì…˜ ì—…ë°ì´íŠ¸
            images = [f.name for f in insta_dir.iterdir() if f.suffix.lower() in [".jpg", ".png", ".webp"]]
            create_toggle_structure(page_id, name, {"caption": caption, "images": images}, {})
            update_checkboxes(page_id, True, False)
        else:
            print(f"   âŒ ìº¡ì…˜ ì¶”ì¶œ ì‹¤íŒ¨")
            insta_fail += 1

        # Rate limiting
        time.sleep(1)

    # 4. ë¸”ë¡œê·¸ ìº¡ì…˜ ë³µêµ¬
    print("\n" + "â”" * 60)
    print("ğŸ“ ë¸”ë¡œê·¸ ìº¡ì…˜ ë³µêµ¬")
    print("â”" * 60)

    blog_success = 0
    blog_fail = 0

    for i, item in enumerate(blog_pages):
        num = item["num"]
        name = item["name"]
        url = item["url"]
        page_id = item["page_id"]

        print(f"\n[{i+1}/{len(blog_pages)}] #{num} {name}")
        print(f"   URL: {url[:50]}...")

        folder = find_content_folder(num)
        if not folder:
            print(f"   âš ï¸ í´ë” ì—†ìŒ")
            blog_fail += 1
            continue

        # 2026-02-13: í”Œë« êµ¬ì¡° ë°˜ì˜
        blog_dir = folder / "02_Blog"
        blog_dir.mkdir(exist_ok=True)
        caption_file = blog_dir / "caption.txt"

        # ì´ë¯¸ ìº¡ì…˜ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if caption_file.exists() and caption_file.stat().st_size > 10:
            print(f"   âœ… ìº¡ì…˜ ì´ë¯¸ ì¡´ì¬ ({caption_file.stat().st_size}ì)")
            blog_success += 1
            update_checkboxes(page_id, False, True)
            continue

        # ìŠ¤í¬ë˜í•‘
        caption = scrape_naver_blog_content(url)

        if caption and len(caption) > 10:
            caption_file.write_text(caption)
            print(f"   âœ… ìº¡ì…˜ ì €ì¥ ({len(caption)}ì)")
            blog_success += 1
            update_checkboxes(page_id, False, True)
        else:
            print(f"   âŒ ìº¡ì…˜ ì¶”ì¶œ ì‹¤íŒ¨")
            blog_fail += 1

        time.sleep(1)

    # 5. ì „ì²´ í˜ì´ì§€ ë…¸ì…˜ êµ¬ì¡° ì—…ë°ì´íŠ¸ (ìº¡ì…˜ ì—†ëŠ” ê²ƒë„)
    print("\n" + "â”" * 60)
    print("ğŸ”„ ë…¸ì…˜ í˜ì´ì§€ êµ¬ì¡° ì—…ë°ì´íŠ¸")
    print("â”" * 60)

    structure_updated = 0

    for i, page in enumerate(pages):
        props = page.get("properties", {})
        num = get_property_value(props, "ë²ˆí˜¸", "number")
        name = get_property_value(props, "ì œëª©", "title") or get_property_value(props, "Name", "title")
        page_id = page["id"]

        if num is None:
            continue

        folder = find_content_folder(num)
        if not folder:
            continue

        # 2026-02-13: í”Œë« êµ¬ì¡° ë°˜ì˜
        insta_dir = folder / "01_Insta&Thread"
        blog_dir = folder / "02_Blog"

        insta_data = {"caption": "", "images": []}
        blog_data = {"caption": "", "images": []}

        if insta_dir.exists():
            caption_file = insta_dir / "caption.txt"
            if caption_file.exists():
                insta_data["caption"] = caption_file.read_text()
            insta_data["images"] = [f.name for f in insta_dir.iterdir()
                                    if f.is_file() and f.suffix.lower() in [".jpg", ".png", ".webp", ".jpeg"]]

        if blog_dir.exists():
            caption_file = blog_dir / "caption.txt"
            if caption_file.exists():
                blog_data["caption"] = caption_file.read_text()
            blog_data["images"] = [f.name for f in blog_dir.iterdir()
                                   if f.is_file() and f.suffix.lower() in [".jpg", ".png", ".webp", ".jpeg"]]

        # ë°ì´í„°ê°€ ìˆìœ¼ë©´ í† ê¸€ êµ¬ì¡° ìƒì„±
        if insta_data["caption"] or insta_data["images"] or blog_data["caption"] or blog_data["images"]:
            if create_toggle_structure(page_id, name, insta_data, blog_data):
                structure_updated += 1
                update_checkboxes(page_id, bool(insta_data["caption"]), bool(blog_data["caption"]))

        if (i + 1) % 20 == 0:
            print(f"   ì§„í–‰: {i+1}/{len(pages)}")

    # 6. ê²°ê³¼ ë¦¬í¬íŠ¸
    print("\n" + "â”" * 60)
    print("ğŸ“Š ì™„ë£Œ ë¦¬í¬íŠ¸")
    print("â”" * 60)
    print(f"ğŸ“¸ ì¸ìŠ¤íƒ€ ìº¡ì…˜:")
    print(f"   - ëŒ€ìƒ: {len(insta_pages)}ê°œ")
    print(f"   - ì„±ê³µ: {insta_success}ê°œ")
    print(f"   - ì‹¤íŒ¨: {insta_fail}ê°œ")
    print(f"\nğŸ“ ë¸”ë¡œê·¸ ìº¡ì…˜:")
    print(f"   - ëŒ€ìƒ: {len(blog_pages)}ê°œ")
    print(f"   - ì„±ê³µ: {blog_success}ê°œ")
    print(f"   - ì‹¤íŒ¨: {blog_fail}ê°œ")
    print(f"\nğŸ”„ ë…¸ì…˜ êµ¬ì¡° ì—…ë°ì´íŠ¸: {structure_updated}ê°œ")
    print("â”" * 60)


if __name__ == "__main__":
    main()
