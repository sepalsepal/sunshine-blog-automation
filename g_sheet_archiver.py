import gspread
from datetime import datetime
import json
import os
from dotenv import load_dotenv

load_dotenv()

# âš ï¸ [ì£¼ì˜] êµ¬ê¸€ ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ì´ ìžˆì–´ì•¼ ìž‘ë™í•©ë‹ˆë‹¤.
# íŒŒì¼ëª…: google_service_account.json
SERVICE_ACCOUNT_FILE = 'google_service_account.json'
SHEET_NAME = os.getenv("ARCHIVE_SHEET_NAME", "Antigravity_Post_Archive")

import streamlit as st

def _get_sheet_client():
    try:
        # 1. ë¡œì»¬ íŒŒì¼ í™•ì¸
        if os.path.exists(SERVICE_ACCOUNT_FILE):
            gc = gspread.service_account(filename=SERVICE_ACCOUNT_FILE)
            return gc
            
        # 2. Streamlit Cloud Secrets í™•ì¸
        if "google_credentials" in st.secrets:
            # st.secretsëŠ” AttrDict í˜•íƒœì¼ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ dictë¡œ ë³€í™˜
            creds_dict = dict(st.secrets["google_credentials"])
            gc = gspread.service_account_from_dict(creds_dict)
            return gc
            
        print(f"âš ï¸ ì¸ì¦ íŒŒì¼ '{SERVICE_ACCOUNT_FILE}'ì´ ì—†ê³ , Secrets ì„¤ì •ë„ ì—†ìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì¸ì¦ ì—ëŸ¬: {e}")
        return None

def _get_or_create_worksheet(gc, sheet_name):
    try:
        sh = gc.open(sheet_name)
    except gspread.SpreadsheetNotFound:
        sh = gc.create(sheet_name)
        print(f"âœ… ìƒˆ ì‹œíŠ¸ ìƒì„±ë¨: {sh.url}")
        # (ì£¼ì˜: ì„œë¹„ìŠ¤ ê³„ì • ì´ë©”ì¼ë¡œ ìƒì„±ë˜ë¯€ë¡œ, ë³¸ì¸ ê³„ì •ì— ê³µìœ í•´ì•¼ í•¨)
    return sh.sheet1

def archive_post(title, content, link, topic):
    """ê²Œì‹œë¬¼ ì •ë³´ ì €ìž¥"""
    gc = _get_sheet_client()
    
    archive_data = {
        "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "topic": topic,
        "title": title,
        "link": link
    }
    
    if not gc:
        # Google Sheets ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ë°±ì—…
        return _local_backup(archive_data)

    try:
        wks = _get_or_create_worksheet(gc, SHEET_NAME)
        
        # í—¤ë” í™•ì¸ ë° ìƒì„±
        header = ["ID", "Date", "Topic", "Title", "Link"]
        if wks.row_values(1) != header:
            wks.insert_row(header, 1)

        # ë°ì´í„° ì¶”ê°€
        next_id = len(wks.get_all_values()) # ê°„ë‹¨ ID ìƒì„±
        row = [
            next_id,
            archive_data["date"],
            topic,
            title,
            link
        ]
        wks.append_row(row)
        print(f"âœ… ì•„ì¹´ì´ë¹™ ì„±ê³µ: {title}")
        return True
        
    except Exception as e:
        print(f"âŒ ì•„ì¹´ì´ë¹™ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ë°±ì—…
        return _local_backup(archive_data)


def _local_backup(data):
    """
    Google Sheets ì‹¤íŒ¨ ì‹œ ë¡œì»¬ JSON ë°±ì—…
    íŒŒì¼: archive_backup.json
    """
    BACKUP_FILE = "archive_backup.json"
    
    try:
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        if os.path.exists(BACKUP_FILE):
            with open(BACKUP_FILE, 'r', encoding='utf-8') as f:
                backup_list = json.load(f)
        else:
            backup_list = []
        
        # ë°ì´í„° ì¶”ê°€
        data['id'] = len(backup_list) + 1
        backup_list.append(data)
        
        # ì €ìž¥
        with open(BACKUP_FILE, 'w', encoding='utf-8') as f:
            json.dump(backup_list, f, ensure_ascii=False, indent=2)
        
        print(f"ðŸ’¾ ë¡œì»¬ ë°±ì—… ì €ìž¥ë¨: {BACKUP_FILE} (ì´ {len(backup_list)}ê°œ)")
        return True
        
    except Exception as e:
        print(f"âŒ ë¡œì»¬ ë°±ì—…ë„ ì‹¤íŒ¨: {e}")
        return False


def get_backup_count():
    """ë¡œì»¬ ë°±ì—… ê°œìˆ˜ í™•ì¸"""
    BACKUP_FILE = "archive_backup.json"
    if os.path.exists(BACKUP_FILE):
        with open(BACKUP_FILE, 'r', encoding='utf-8') as f:
            return len(json.load(f))
    return 0


def get_statistics():
    """
    í†µê³„ ë°ì´í„° ì¡°íšŒ (ëŒ€ì‹œë³´ë“œìš©)
    Returns: {
        'total_posts': int,
        'posts_this_month': int,
        'posts_this_week': int,
        'recent_posts': list[dict],  # ìµœê·¼ 5ê°œ
        'source': 'sheets' | 'backup' | 'none'
    }
    """
    from datetime import datetime, timedelta
    
    stats = {
        'total_posts': 0,
        'posts_this_month': 0,
        'posts_this_week': 0,
        'recent_posts': [],
        'source': 'none'
    }
    
    all_posts = []
    
    # 1. Google Sheetsì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    gc = _get_sheet_client()
    if gc:
        try:
            wks = _get_or_create_worksheet(gc, SHEET_NAME)
            records = wks.get_all_records()
            all_posts = records
            stats['source'] = 'sheets'
        except Exception as e:
            print(f"âš ï¸ ì‹œíŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    # 2. ë¡œì»¬ ë°±ì—…ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (fallback ë˜ëŠ” ë³‘í•©)
    BACKUP_FILE = "archive_backup.json"
    if os.path.exists(BACKUP_FILE):
        try:
            with open(BACKUP_FILE, 'r', encoding='utf-8') as f:
                backup_data = json.load(f)
                if not all_posts:
                    all_posts = backup_data
                    stats['source'] = 'backup'
        except:
            pass
    
    if not all_posts:
        return stats
    
    # í†µê³„ ê³„ì‚°
    now = datetime.now()
    month_start = now.replace(day=1, hour=0, minute=0, second=0)
    week_start = now - timedelta(days=now.weekday())
    
    for post in all_posts:
        try:
            date_str = post.get('Date') or post.get('date', '')
            if date_str:
                post_date = datetime.strptime(date_str[:10], '%Y-%m-%d')
                
                if post_date >= month_start:
                    stats['posts_this_month'] += 1
                if post_date >= week_start:
                    stats['posts_this_week'] += 1
        except:
            pass
    
    stats['total_posts'] = len(all_posts)
    stats['recent_posts'] = all_posts[-5:][::-1]  # ìµœê·¼ 5ê°œ, ì—­ìˆœ
    
    return stats


